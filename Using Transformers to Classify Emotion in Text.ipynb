{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transformers to Classify Emotion in Text\n",
    "\n",
    "This project began as a capstone for the Udacity Machine Learning Engineer Nanodegree is described in more detail in this <a href=\"https://drive.google.com/open?id=1ALVUnQS1eLqQHcMiU0uMD2xOnCuGMe8x\">project proposal</a>.\n",
    "\n",
    "I then expanded upon the tuning and analysis of the results over time in an attempt to improve the model.\n",
    "\n",
    "## Project Summary \n",
    "\n",
    "As a Psychologist who is interested in analyzing the content and outcomes of mental health treatment, text data is very important. Most of the content of clinical documentation is currently stored in text, and the content of psychotherapy could be either manually transcribed or automatically via speech-to-text. Thus, I'm very interested in using modern Natural Language Processing (NLP) techniques to classify the content of text.\n",
    "\n",
    "In this project, I'm attempting to see if a modern NLP technique called <a href=\"https://arxiv.org/pdf/1810.04805.pdf\"> \"Bidirectional Encoder Representations from Transformers\" (BERT)</a> or similar Transformer based models can be used to improve upon previous attempts in the literature to classify emotion from text.\n",
    "\n",
    "The study I am using as a baseline is entitled <a href=\"https://www.aclweb.org/anthology/C18-1179.pdf\">\"An Analysis of Annotated Corpora for Emotion Classification in Text\"</a>. This study looked at a number of different text corpora which had been labeled with expressed emotion. The dataset I selected from the study is called <a href=\"https://arxiv.org/pdf/1710.03957.pdf\">\"DailyDialog\"</a>. I selected this dataset for a few reasons:\n",
    "\n",
    "1. I am intending to do a within corpus prediction, and this dataset was noted as one of the best for that purpose in the study. They were able to achieve an F1 score of .71, which was one of the higher within corpus results.\n",
    "2. It contains content that is meant to represent usual discussions between individuals, back and forth, which would be a better facsimile of a therapy session.\n",
    "3. It was readily and publicly available for download.\n",
    "4. Upon initial review, it looked quite clean and well documented, requiring a minimum amount of data transformation and cleaning.\n",
    "5. It contains multiple labeled emotions (6 discrete emotions and \"no emotion\"), which makes it a more challenging prediction task than simple positive or negative binary emotion.\n",
    "\n",
    "## Goal\n",
    "\n",
    "My goal is to use BERT (or a BERT-like model) to predict the labeled emotion for each sentence in the dataset. I hope to exceed the study's F1 score of .71 and will be using F1 because the dataset is relatively unbalanced in its class representation, meaning pure accuracy would not be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Data and Initial Exploration\n",
    "\n",
    "The data is stored in two separate raw text files (dialogues_text.txt and dialogues_emotion.txt). The former contains the text of the conversations, while the latter contains the labeled emotions.\n",
    "\n",
    "The emotion classes are labeled numerically:\n",
    "\n",
    "- 0: 'no emotion'\n",
    "- 1: 'anger' \n",
    "- 2: 'disgust'\n",
    "- 3: 'fear' \n",
    "- 4: 'happiness' \n",
    "- 5: 'sadness'\n",
    "- 6: 'surprise'\n",
    "\n",
    "In the cell below I load in the two files to show how the data is represented. As you can see, each sentence is separated by a marker \"__eou__\" and each conversation is separated by a new line.\n",
    "\n",
    "For emotions, each number is an emotion label corresponding to a single sentence in the text file, separated by spaces, and with conversations again split by a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: \n",
      "The kitchen stinks . __eou__ I'll throw out the garbage . __eou__\n",
      "So Dick , how about getting some coffee for tonight ? __eou__ Coffee ? I don ’ t honestly like that kind of stuff . __eou__ Come on , you can at least try a little , besides your cigarette . __eou__ What ’ s wrong with that ? Cigarette is the thing I go crazy for . __eou__ Not for me , Dick . __eou__\n",
      "Are things still going badly with your houseguest ? __eou__ Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . __eou__ Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law . __eou__ You ’ re right . Everything is probably going to come to a head toni\n",
      "\n",
      "\n",
      "Sample Emotion Labels: \n",
      "2 0 \n",
      "4 2 0 1 0 \n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries.\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "#Load in each text file\n",
    "dialogues_text = open(\"dialogues_text.txt\", encoding=\"utf-8\").read()\n",
    "dialogues_emotion = open(\"dialogues_emotion.txt\", encoding=\"utf-8\").read()\n",
    "\n",
    "print(\"Sample Text: \\n\" + dialogues_text[0:1000] + \"\\n\\n\")\n",
    "print(\"Sample Emotion Labels: \\n\" + dialogues_emotion[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Manipulation\n",
    "\n",
    "The data needs to be both transformed and loaded into a Pandas DataFrame for the eventual model training. We are looking for the eventual model training, the data needs to be in a DataFrame with two columns, one entitled \"text\", which has one row per sentence, and one entitled \"label\", with each label represented by a number.\n",
    "\n",
    "Our data is relatively close to that format. In order to get the text data into one sentence per line, the data will first have all newlines removed, as they are irrelevant to this analysis, and then the data will be split based on the special \"\\_\\_eou\\_\\_\" character which represents the end of a sentence.\n",
    "\n",
    "For the emotion labels, again all newline characters will be removed, and then the data will simply be split by spaces.\n",
    "\n",
    "These will both be loaded into separate arrays and then put together into the final dataframe.\n",
    "\n",
    "As you can see below, we end up with 102981 sentences labeled with emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102981\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries.\n",
    "import pandas as pd\n",
    "\n",
    "#load both files into arrays, stripping out the new-line characters and splitting.\n",
    "dialogues_text = open(\"dialogues_text.txt\", encoding=\"utf-8\").read().replace('\\n', ' ').split(' __eou__ ')\n",
    "dialogues_emotion = open(\"dialogues_emotion.txt\", encoding=\"utf-8\").read().replace('\\n', '').split(' ')\n",
    "\n",
    "#After some analysis it was determined that for some reason a single emotion label was missing from the dataset at this point.\n",
    "#The sentence appeared to have no emotion, and this class is the most common, so would have the least impact on the data,\n",
    "#so a zero is inserted in the emotion array here.\n",
    "dialogues_emotion.insert(4632, '0')\n",
    "\n",
    "print(len(dialogues_text))\n",
    "\n",
    "#Load both arrays into a single dataframe, remove any rows that have blank sentences and convert the label column to an integer\n",
    "#from the original string data type.\n",
    "dialogues_df = pd.DataFrame({'text': dialogues_text, 'labels': dialogues_emotion})\n",
    "dialogues_df = dialogues_df[dialogues_df.text != '']\n",
    "dialogues_df.labels = dialogues_df.labels.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  labels\n",
      "0                               The kitchen stinks .       2\n",
      "1                       I'll throw out the garbage .       0\n",
      "2  So Dick , how about getting some coffee for to...       4\n",
      "3  Coffee ? I don ’ t honestly like that kind of ...       2\n",
      "4  Come on , you can at least try a little , besi...       0\n",
      "1412\n"
     ]
    }
   ],
   "source": [
    "#Examining the head of the DataFrame, we see that the text and labels loaded correctly.\n",
    "#The longest sentence is 1412 characters, which may affect the ideal sequence length in our eventual model.\n",
    "print(dialogues_df.head())\n",
    "print(dialogues_df.text.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Distribution\n",
    "\n",
    "It's important to take a look at the distribution of the dataset, as imbalanced datasets can sometimes affect certain models, and it definitely impacts the evaluation metric used. If one class is overly represented, for example, and the model just learns to always predict that class, it can score quite well despite having very little predictive power.\n",
    "\n",
    "In our dataset, as shown below, around 83% of the sentences as labeled with \"no emotion\". If our model predicted no emotion for every sentence, it would have an accuracy score of 83%, which is not bad, but obviously not a well trained model.\n",
    "\n",
    "As such, we'll be using the F1 score for this model, which balances valuing precision and recall. In addition, we'll be using the F1 micro score, partially because that is what was used in the original study we are using as a benchmark, and also because it averages the F1 scores achieved for each class, so even very small classes can be equally represented in the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Emotion: 85573  (83.1%)\n",
      "Anger: 1022  (1.0%)\n",
      "Disgust: 353  (0.3%)\n",
      "Fear: 174  (0.2%)\n",
      "Happiness: 12885  (12.5%)\n",
      "Sadness: 1150  (1.117%)\n",
      "Surprise: 1823  (1.77%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#Calculate the counts of each emotion in the dataset\n",
    "total_emotions_count = len(dialogues_df.labels)\n",
    "no_emotion_count = len(dialogues_df[dialogues_df.labels == 0])\n",
    "anger_count = len(dialogues_df[dialogues_df.labels == 1])\n",
    "disgust_count = len(dialogues_df[dialogues_df.labels == 2])\n",
    "fear_count = len(dialogues_df[dialogues_df.labels == 3])\n",
    "happiness_count = len(dialogues_df[dialogues_df.labels == 4])\n",
    "sadness_count =  len(dialogues_df[dialogues_df.labels == 5])\n",
    "surprise_count =  len(dialogues_df[dialogues_df.labels == 6])\n",
    "\n",
    "#Print the count along with the precentage of the total\n",
    "print('No Emotion: ' + str(no_emotion_count) + \"  (\" + str(round(no_emotion_count/total_emotions_count, 3)*100) + \"%)\")\n",
    "print('Anger: ' + str(anger_count) + \"  (\" + str(round(anger_count/total_emotions_count, 3)*100) + \"%)\")\n",
    "print('Disgust: ' + str(disgust_count) + \"  (\" + str(round(disgust_count/total_emotions_count, 3)*100) + \"%)\")\n",
    "print('Fear: ' + str(fear_count) + \"  (\" + str(round(fear_count/total_emotions_count, 3)*100) + \"%)\")\n",
    "print('Happiness: ' + str(happiness_count) + \"  (\" + str(round(happiness_count/total_emotions_count, 3)*100) + \"%)\")\n",
    "print('Sadness: ' + str(sadness_count) + \"  (\" + str(round(sadness_count/total_emotions_count, 5)*100) + \"%)\")\n",
    "print('Surprise: ' + str(surprise_count) + \"  (\" + str(round(surprise_count/total_emotions_count, 4)*100) + \"%)\")\n",
    "\n",
    "#Use seaborn to visualize the distribution.\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.countplot(x=\"labels\", data=dialogues_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show the distribution of the lengths of each sentence in number of characters. As we can see, the data is heavily skewed, with a long tail to the right, from the min length of 1 character all the way to 1412 characters in one sentence. The mean is around 60 characters, the median at 47, and the histogram shows that almost all sentences fall within 0 to 200  characters or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 60.09\n",
      "Median Length: 47.0\n",
      "Max Length: 1412\n",
      "Min Length: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bbbb98fcc8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialogues_length = dialogues_df.text.map(lambda x: len(x))\n",
    "print(\"Mean Length: \" + str(round(dialogues_length.mean(),2)))\n",
    "print(\"Median Length: \" + str(dialogues_length.median()))\n",
    "print(\"Max Length: \" + str(dialogues_length.max()))\n",
    "print(\"Min Length: \" + str(dialogues_length.min()))\n",
    "\n",
    "sns.distplot(dialogues_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data into Train and Evaluation Datasets\n",
    "\n",
    "Now that the data has been tranformed appropriately and we understand the distribution, we need to split the dataset into training, evaluation and test datasets. The original study used 80%/20% train/test split, but I'll be using a 70%/15%/15% train/eval/test split. The eval split will be used to select a model, validate the model performance during training and test hyperparamter tuning. The test set will be held out to only provide final performance metrics. This should give a more accurate expectation of how the model would generalize to new (similar) data and if anything will make it harder for me to achieve the evaluation metrics that the study achieved.\n",
    "\n",
    "## Data Imbalance\n",
    "\n",
    "As we saw above, the data is heavily imabalanced. It's important that the proportion of the classes is similar in the training, evaluation and test datasets. So, first I'll split the data by class, then distribute the same proportion of the data randomly to the three datasets.\n",
    "\n",
    "I've left code below to apply a \"none\" label to any data beyond the selected train/eval/test data, as this makes it easy to only select .07, .015 and .015 for example to work with 10% of the data for quicker training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train by emotion\n",
    "# Create dict\n",
    "by_emotion = collections.defaultdict(list)\n",
    "for _, row in dialogues_df.iterrows():\n",
    "    by_emotion[row.labels].append(row.to_dict())\n",
    "    \n",
    "# Create split data\n",
    "final_list = []\n",
    "np.random.seed(42)\n",
    "for _, item_list in sorted(by_emotion.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(.70*n)\n",
    "    n_val = int(.15*n)\n",
    "    n_test = int(.15*n)\n",
    "    \n",
    "    #Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "        item['split'] = 'test'\n",
    "    for item in item_list[n_train+n_val+n_test:]:\n",
    "        item['split'] = 'none'  \n",
    "    \n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "I created a pre-processing function with common methods of processing textual data using regular expressions (making the text data lowercased, inserting spaces around punctionation, any removing any uncommon characters or puncuation), but I didn't end up using it. The data from the study has already had some light pre-processing (spacing etc), applied and the transformers library I will be using I believe performs necessary text pre-processing and tokenization as necessary for BERT-like models by itself.\n",
    "\n",
    "Instead, I simply split the data using the applied labels above into Train, Eval and Test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 72084\n",
      "Eval: 15443\n",
      "Test: 15443\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(filter(lambda x: x['split'] == 'train', final_list))\n",
    "eval_df = pd.DataFrame(filter(lambda x: x['split'] == 'val', final_list))\n",
    "test_df = pd.DataFrame(filter(lambda x: x['split'] == 'test', final_list))\n",
    "\n",
    "\n",
    "print(\"Train: {}\".format(len(train_df)))\n",
    "print(\"Eval: {}\".format(len(eval_df)))\n",
    "print(\"Test: {}\".format(len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Training\n",
    "\n",
    "For this project, I decided to use the [\"Simple Transformers\" library](https://github.com/ThilinaRajapakse/simpletransformers).\n",
    "\n",
    "After researching BERT a good amount, it seemed like one of the more popular libraries for applying BERT's pre-trained models was the [\"Pytorch-Transformers\" library by HuggingFace](https://github.com/huggingface/transformers). HuggingFace has apparently done a great job of exposing a number of pre-trained large NLP transformers through a consistent Pytorch interface.\n",
    "\n",
    "Simple Transformers then took some of the more popular transformers and created an even higher level and user friendly wrapper for the Pytorch-Transformers library. Many BERT, XLNet, NLM, RoBERTa and DistilBERT models are available. The primary benefit of this library is that it's very user friendly, as seen below, only requiring data in a particular format, and then allowing for training, evaluation and prediction in 3 lines of code.\n",
    "\n",
    "Despite that simplicity, it can handle a number of different application of transformers including multi-class classification (as I'm doing here). In addition, one can pass a number of parameters to the training and evaluation objects to modify hyperparameters such as maximum sequence length, batch size, learning rate and warmup ratio among others.\n",
    "\n",
    "For my model, I experimented a few different BERT models, beginning with RoBERTa-base, then using BERT cased, XLNet, DistilBERT abd DistilRoBERTa. After running all these models against a 10% subset of the data, I ended up landing on DistilRoBERTa (which is a distilled down, smaller RoBERTa pre-trained model). Being a distilled model, it trains quicker, achieved the highest Matthews Correlation Coefficient (mcc) and also seemed to be more resistant to over-fitting in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model Tuning\n",
    "\n",
    "After training the model using default hyper-parameters, I noticed a few things. Firstly, the training loss appeared to fluctuate wildly. I also noticed that it appeared to achieve a general minimum of loss relatively early in the training, and plateau or even increase. Lastly, on evaluation, I noticed that the model was only predicting 4 of the 7 classes, yet still achieving a high (~ 87%) accuracy and a decent mcc score (~ .47). Given that the dataset nearly 96% of the observations in the dataset are classified as either No Emotion or Happiness, this made sense, but seemed like a distinct case of overfitting to the training data.\n",
    "\n",
    "In order to diagnose this, I implemented a tool called <a href=\"https://www.wandb.com/\">Weights and Biases</a>, which allows for real-time model evaluation and is supported by the Simple Transformers library. I had originally experimented with TensorBoard, which provided similar metrics, but was not as user-friendly nor did it update seamlessly as the training was occurring. As a test, I initially ran against 10% of the data, 1 epoch only and received the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Loss Fluctuations](images/Training%20Loss%20Fluctuations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of things to note here, but the one thing that jumped out in particular was the very large fluctuations in the training loss (and mcc). After doing a bit of research, it seemed like a common cause for this is that the batch size is set too low. When the model is performing adjusting weights through back propagation based on the loss generated from only a few samples of the data, it can swing too far in one direction or the other.\n",
    "\n",
    "I experimented with a few different settings, but batch rates that were too high (256 for example) caused my poor 8GB GPU to immediately run out of memory. I was able to sometimes get 128 to work, but settled on using 64 for reliability and ran the same training again, but with 10 epochs instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overfitting](images/Overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in batch size did appear to address the large fluctuations in training loss and mcc at least somewhat.\n",
    "\n",
    "As you can see, this tool also illustrated a lot of useful things about this model. First, you can see the learning rate quickly rise up, then slowly drop, which shows the library itself is allowing for a flexible learning rate as it begins to find a minimum which should require less manual tuning of the learning rate parameter.\n",
    "\n",
    "Secondly, we can see the training loss continue to decrease throughout the 10 epochs, but the loss on the evaluation dataset begin to increase consistently after about 50 steps.\n",
    "\n",
    "Next, I attempted to address this issue of overfitting to the training set and increase in evaluation loss by implementing Early Stopping. Simple Transformers has parameters to enable early stopping of training once it detects that the evaluation loss has been increasing consistently for a period of time. I could also continue to explore further methods of avoiding overfitting using regularization (such as L2 regularization and dropout).\n",
    "\n",
    "What follows in the code I used to train this final version of the model, with 3 epochs and early stopping enabled. I also adjusted the Weights and Biases logging to occur only every 100 steps to speed up model training. The Weights and Biases visuals follow immediately after.\n",
    "\n",
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e34157fecf4e30af51e10fd0a15d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('DLL load failed: The specified procedure could not be found.')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d76ad49539440ea9a28598287b543f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/brinew27/visualization-demo\" target=\"_blank\">https://app.wandb.ai/brinew27/visualization-demo</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/brinew27/visualization-demo/runs/qgop1j40\" target=\"_blank\">https://app.wandb.ai/brinew27/visualization-demo/runs/qgop1j40</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4bce6a642948f5bcd02b2af50a1dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1127.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.992396"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.314169Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 0.452754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.510146\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.241458\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.455666\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.476173\n",
      "No improvement in eval_loss for 2 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.318784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97152975e4774f22a440719b2d25ec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1127.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.393278\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.243230\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.303280\n",
      "No improvement in eval_loss for 2 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.267167\n",
      "No improvement in eval_loss for 3 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.202245\n",
      "Patience of 3 steps reached.\n",
      "Training terminated.\n",
      "\n",
      "Training of roberta model complete. Saved to outputs/.\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "train_args = {\n",
    "    'evaluate_during_training': True,\n",
    "    'logging_steps': 100,\n",
    "    'num_train_epochs': 3,\n",
    "    'evaluate_during_training_steps': 100,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'train_batch_size': 64,\n",
    "    'eval_batch_size': 64,\n",
    "    'overwrite_output_dir': True,\n",
    "    'use_cached_eval_features' : False,\n",
    "    'reprocess_input_data': True,\n",
    "    'wandb_project': \"visualization-demo\",\n",
    "    \"use_early_stopping\": True,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"early_stopping_delta\": 0\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('roberta', 'distilroberta-base', num_labels=7, args=train_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, eval_df=eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Early Stopping](images/Early%20Stopping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the model stopped early (about 1.5 epochs in) once the evaluation loss started to climb consistently for 3 iterations.\n",
    "\n",
    "The training loss was at its lowest point, and the mcc had achieved its highest value yet of nearly .51!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Firstly, I created a couple of small custom functions for evaluation. Simple Transformers enables you to pass any evaluation metric into its .eval_model() function as long as it takes a list of predictions and true values as parameters (which is the norm for most Scikit Learn metrics), or create your own.\n",
    "\n",
    "I first created a function using Scikit-Learn's F1 Score Micro calculation, since this is the metric that was used in the original study. I personally believe there are some issues with this, since as far as I understand it, micro F1 in this case will be the same as the Accuracy, which is not a very useful metric due the very large class imablance (a model that simply guessed \"No Emotion\" every time would achieve a very respectable accuracy).\n",
    "\n",
    "I also created a custom confusion matrix function that will display, for each of the predicted answers, the % of the time it was in fact each true answer, which makes it easy to see specific areas where the model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_score(y_true, y_pred):\n",
    "    return sklearn.metrics.f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    # create some data\n",
    "    lookup = {0: 'no emotion', 1: 'anger', 2: 'disgust', 3: 'fear', 4: 'happiness', 5: 'sadness', 6: 'surprise'}\n",
    "    y_true = pd.Series([lookup[_] for _ in y_true])\n",
    "    y_pred = pd.Series([lookup[_] for _ in y_pred])\n",
    "\n",
    "    return pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted']).apply(lambda r: 100.0 * r/r.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d425d1322a146b8a1fa0b4dbec4a7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15443.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2128393de4284f73980defd7d26b8547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.5085466329944889, 'eval_loss': 0.3814309377623491, 'f1': 0.8672537719355048, 'acc': 0.8672537719355048, 'conf': Predicted       anger    disgust   fear  happiness  no emotion    sadness  \\\n",
      "True                                                                        \n",
      "anger       35.483871  14.285714    0.0   0.236407    0.899753   1.538462   \n",
      "disgust     11.290323  57.142857    0.0   0.000000    0.232436   0.000000   \n",
      "fear         0.000000   0.000000    0.0   0.059102    0.172453   1.538462   \n",
      "happiness    6.451613   0.000000    0.0  64.361702    6.185799   3.076923   \n",
      "no emotion  41.935484  19.047619  100.0  34.456265   90.687561  58.461538   \n",
      "sadness      1.612903   9.523810    0.0   0.118203    1.057209  32.307692   \n",
      "surprise     3.225806   0.000000    0.0   0.768322    0.764790   3.076923   \n",
      "\n",
      "Predicted    surprise  \n",
      "True                   \n",
      "anger        1.132075  \n",
      "disgust      0.754717  \n",
      "fear         0.377358  \n",
      "happiness    4.528302  \n",
      "no emotion  33.207547  \n",
      "sadness      1.886792  \n",
      "surprise    58.113208  }\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=micro_f1_score, acc=sklearn.metrics.accuracy_score, conf=confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis\n",
    "\n",
    "Despite the questionable utility of the Micro F1 score, as you can see from the output above, the model was able to achieve an F1 score of nearly .87 against the evaluation dataset, a large increase over the baseline score of .71 from the original study. We can also see the nearly .51 mcc that was shown in our visuals earlier.\n",
    "\n",
    "Lastly, looking at the confusion matrix, we can see that (despite resolving the issue I described earlier of only predicting 4 of the 7 classes), it continues to consistently over-predict \"No Emotion\". In the case of fear (one of the smallest classes), it predicted no emotion 100% of the time. This makes sense. Even without heavily over-fitting, if the sentence isn't clear, guessing non-emotion has the greatest change of being correct simply due to the distribution.\n",
    "\n",
    "Since this issue was persisting, I decided to not yet evaluate against the Test holdout dataset, and instead implement another measure to attempt to address the imbalanced dataset somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Classes Mitigation\n",
    "\n",
    "From my research, there are a number of ways of addressing class imbalance, including data generation for less frequent classes, or simply reducting the amount of the majority class, or duplicating the smaller classes.\n",
    "\n",
    "These all seemed to have their own drawbacks (reducing the amount of training data in the case of reducing the majority class, and increasing training time in the case of duplicating the minority classes), so I went with setting class weights during training.\n",
    "\n",
    "By giving the classes weights, from my understanding it essentially penalizes incorrect guesses on the minority classes more then the majority classes, by generating a large amount of loss when those minority classes aren't predicted correctly.\n",
    "\n",
    "First, I created variables to hold the weights of each of the classes, simply by dividing the number of observations into the total number. I then used these weights in the \"weights\" parameter and trained again, keeping everything else the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Emotion Weight: 1.2033855862172584\n",
      "Anger Weight: 100.81678321678322\n",
      "Disgust Weight: 291.83805668016197\n",
      "Fear Weight: 595.7355371900826\n",
      "Happiness Weight: 7.992460361459142\n",
      "Sadness Weight: 89.54534161490683\n",
      "Surprise Weight: 56.492163009404386\n"
     ]
    }
   ],
   "source": [
    "total_emotions_count = len(train_df.labels)\n",
    "\n",
    "no_emotion_weight = total_emotions_count/len(train_df[train_df.labels == 0])\n",
    "anger_weight = total_emotions_count/len(train_df[train_df.labels == 1])\n",
    "disgust_weight = total_emotions_count/len(train_df[train_df.labels == 2])\n",
    "fear_weight = total_emotions_count/len(train_df[train_df.labels == 3])\n",
    "happiness_weight = total_emotions_count/len(train_df[train_df.labels == 4])\n",
    "sadness_weight = total_emotions_count/len(train_df[train_df.labels == 5])\n",
    "surprise_weight = total_emotions_count/len(train_df[train_df.labels == 6])\n",
    "\n",
    "print(\"No Emotion Weight: {}\".format(no_emotion_weight))\n",
    "print(\"Anger Weight: {}\".format(anger_weight))\n",
    "print(\"Disgust Weight: {}\".format(disgust_weight))\n",
    "print(\"Fear Weight: {}\".format(fear_weight))\n",
    "print(\"Happiness Weight: {}\".format(happiness_weight))\n",
    "print(\"Sadness Weight: {}\".format(sadness_weight))\n",
    "print(\"Surprise Weight: {}\".format(surprise_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59f473881b343a2be448c80db549668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49c3f1c42ba43ec903354af19779ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/brinew27/visualization-demo\" target=\"_blank\">https://app.wandb.ai/brinew27/visualization-demo</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/brinew27/visualization-demo/runs/5a3qb2kr\" target=\"_blank\">https://app.wandb.ai/brinew27/visualization-demo/runs/5a3qb2kr</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.8.26 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09fdfd16574a7daa6b0ce15a231fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1127.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.752344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.481061Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 0.367568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.349208\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.255422\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.250587\n",
      "No improvement in eval_loss for 2 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.214322\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd680cdc682a4e25b2d33074d2aa78df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1127.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.308047\n",
      "No improvement in eval_loss for 1 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.513413\n",
      "No improvement in eval_loss for 2 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.151663\n",
      "No improvement in eval_loss for 3 steps.\n",
      "Training will stop at 3 steps.\n",
      "\n",
      "Running loss: 0.316634\n",
      "Patience of 3 steps reached.\n",
      "Training terminated.\n",
      "\n",
      "Training of roberta model complete. Saved to outputs/.\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "train_args = {\n",
    "    'evaluate_during_training': True,\n",
    "    'logging_steps': 100,\n",
    "    'num_train_epochs': 3,\n",
    "    'evaluate_during_training_steps': 100,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'train_batch_size': 64,\n",
    "    'eval_batch_size': 64,\n",
    "    'overwrite_output_dir': True,\n",
    "    'use_cached_eval_features' : False,\n",
    "    'reprocess_input_data': True,\n",
    "    'wandb_project': \"visualization-demo\",\n",
    "    \"use_early_stopping\": True,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"early_stopping_delta\": 0,\n",
    "    \"weights\": [no_emotion_weight, anger_weight, disgust_weight, fear_weight, happiness_weight, sadness_weight, surprise_weight]\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('roberta', 'distilroberta-base', num_labels=7, args=train_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, eval_df=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90455b044ebb4d09a7d65af0fb01e16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15443.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58fab64568b47378393514cca0a437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.5063363410832458, 'eval_loss': 0.38723472256428937, 'f1': 0.8646636016318073, 'acc': 0.8646636016318073, 'conf': Predicted       anger    disgust   fear  happiness  no emotion    sadness  \\\n",
      "True                                                                        \n",
      "anger       45.000000  14.285714    0.0   0.372539    0.824508   0.000000   \n",
      "disgust     15.000000  50.000000    0.0   0.053220    0.249622   0.000000   \n",
      "fear         1.666667   0.000000    0.0   0.053220    0.166415  16.666667   \n",
      "happiness    0.000000   0.000000    0.0  60.936668    5.832073   0.000000   \n",
      "no emotion  35.000000  35.714286  100.0  37.519957   90.930408  50.000000   \n",
      "sadness      0.000000   0.000000    0.0   0.053220    1.232980  33.333333   \n",
      "surprise     3.333333   0.000000    0.0   1.011176    0.763994   0.000000   \n",
      "\n",
      "Predicted    surprise  \n",
      "True                   \n",
      "anger        3.041825  \n",
      "disgust      0.760456  \n",
      "fear         0.380228  \n",
      "happiness    6.083650  \n",
      "no emotion  30.038023  \n",
      "sadness      2.281369  \n",
      "surprise    57.414449  }\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=micro_f1_score, acc=sklearn.metrics.accuracy_score, conf=confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0c4410634848f2afd6628fa327d9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15443.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff05afab5d114b2183960603e6d66ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=242.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.5193759128992298, 'eval_loss': 0.3710164073392872, 'f1': 0.8689373826329081, 'acc': 0.8689373826329081, 'conf': Predicted       anger    disgust  happiness  no emotion    sadness   surprise\n",
      "True                                                                         \n",
      "anger       55.737705   8.333333   0.383772    0.739009   0.000000   4.676259\n",
      "disgust      4.918033  58.333333   0.054825    0.294095   0.000000   0.719424\n",
      "fear         0.000000   0.000000   0.000000    0.180982   0.000000   0.719424\n",
      "happiness    0.000000   0.000000  62.335526    5.836664   0.000000   7.553957\n",
      "no emotion  26.229508  33.333333  36.348684   91.048941  57.142857  26.618705\n",
      "sadness      4.918033   0.000000   0.109649    1.221627  42.857143   0.719424\n",
      "surprise     8.196721   0.000000   0.767544    0.678682   0.000000  58.992806}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df, f1=micro_f1_score, acc=sklearn.metrics.accuracy_score, conf=confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis\n",
    "\n",
    "Despite setting the weights, we actually see very little change in the confusion matrix for the evaluation dataset. Some areas were improved slightly (Anger moved from 35% correct to 45%) but others stayed mostly the same or even reduced slightly. The weights will probably require further tweaking and/or other methods of addressing the class imbalance will need to be implemented. However, given that I'd done a moderate amount of tuning to improve the model performance, I decided to run against the holdout Test dataset.\n",
    "\n",
    "I achieved an even higher mcc score (nearly .52) and similar accuracy. We can see from the confusion matrix that for every class, it had a better htan 50% change of picking the right answer, though sadly we do see that the fear predictions again were never made. It would probably be worthwhile to look at the fear predictions that were mis-classified (seemingly most commonly as surprise) to get a better idea of why the model was having a hard time identifying them, as well as further weight the fear class and/or add additional labeled observations of fear emotion sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Lastly, we can use the trained model to make a few sample predictions. As you can see below, the model was able to correctly label my example surprise and happy sentences, but struggled with my anger and disgust examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare a dictionary for clearer class prediction.\n",
    "emotions = {0: 'no emotion', 1: 'anger', 2: 'disgust', 3: 'fear', 4: 'happiness', 5: 'sadness', 6: 'surprise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828025d7a1a54b21b3914f630e04657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae0dafb80a9434a9c9e8466cde8c207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "surprise\n",
      "happiness\n",
      "no emotion\n",
      "no emotion\n",
      "[6 4 0 0]\n"
     ]
    }
   ],
   "source": [
    "to_predict = [\"What??? No way!\", \"I'm so glad you come over.\", \"You need to stop that, I'm getting upset.\", \"Ewww, that looks gross.\"]\n",
    "\n",
    "preds, model_outputs = model.predict(to_predict)\n",
    "\n",
    "for prediction in preds:\n",
    "    print(emotions.get(prediction))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts and Next Steps\n",
    "\n",
    "I was impressed by both the power of even the reduced DistilRoBERTa model as well as the ease of use and flexibility of the Simple Transformers library.\n",
    "\n",
    "One take away from the original study was that the nature of the training data can affect the predictive power of the eventual model dramatically. Despite this being a relatively large dataset (over 100,000 labeled sentences), most classes were heavily under-represented, which likely hurt its ability to accurately predict some of those smaller classes. In addition, simply perusing some of the sample mis-classified sentences made me question some of the labeling, which is a potential byproduct of using humans to manually label so many sentences as a third party As always, getting a robust enough, balanced, clean source of data is a challenge.\n",
    "\n",
    "As next steps, I'd like to deploy this model to AWS through a service like Sagemaker to be able to serve predictions in the cloud and perhaps build a simple web interface to generate predictions against entered text. However, even on my local GPU accelerated machine, predictions take a second or two as features are generated, so this would need to be optimized for actual production usage.\n",
    "\n",
    "Lastly, I'd like to take my learnings from this project and apply them to other mental health text data and explore applications of BERT against actual mental health related interactions to improve treatment delivery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
